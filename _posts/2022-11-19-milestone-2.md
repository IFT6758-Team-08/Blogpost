---
layout: post
title: Milestone 2
---


## <span style="color:SlateBlue;">Q1. Experiment Tracking</span>

The experiment tracking involves the usage of comet.ml to provide information about the models used and logging into the comet session.

## <span style="color:SlateBlue;">Q2. Feature Engineering I</span>

**Question1:  Create and include the following figures in your blogpost and briefly discuss your observations (few sentences):**
- **A histogram of shot counts (goals and no-goals separated), binned by distance**
- **A histogram of shot counts (goals and no-goals separated), binned by angle**
- **A 2D histogram where one axis is the distance and the other is the angle. You do not need to separate goals and no-goals.**

**Answer:**

**The histogram of shot counts (goals and no-goals separated), binned by distance is shown below:**

![shot_count_by_distance_from_net](..\public\Shot_count_by_distance_from_net.png)

The histogram depicts that the number of shots taken by the players is more when the distance from the net is less than 20ft. The number of shots taken by the players is less when the distance from the net is more than 20.

**The histogram of shot counts (goals and no-goals separated), binned by angle is shown below:**
![shot_count_by_angle_from_net](..\public\Shot_count_by_angle_from_net.png)

The above histogram depicts that the number of shots taken by the players is more when the angle from the net is less than 45 degrees. The number of shots taken by the players is less when the angle from the net is more than 45 degrees. The goals are distributed as a normal distribution with 0 degrees shots having the highest number of goals.

**The 2D histogram where one axis is the distance and the other is the angle is shown below:**

![shot_count_by_distance_angle](..\public\Shot_count_by_distance_angle.png)

The above histogram depicts that the number of shots taken by the players is more when the distance from the net is less than 20ft and the angle from the net is less than 45 degrees. The number of shots taken by the players is less when the distance from the net is more than 20ft and the angle from the net is more than 45 degrees. It shows that shots have very less chances of being a goal when the distance from the net is more than 20ft and the angle from the net is more than 45 degrees.

**Question2: Create two more figures relating the goal rate, i.e. #goals / (#no_goals + #goals), to the distance, and goal rate to the angle of the shot. Include these figures in your blogpost and briefly discuss your observations.**

**Answer:**

**The histogram of goal rate, i.e. #goals / (#no_goals + #goals), binned by distance is shown below:**

![m2_q2_2(1)](..\public\m2_q2_2(1).png)

The goal rate is the highest when the distance from the net is less than 30ft and is the lowest when the distance from the net is in the range of 40-80ft. After the shot distance crosses 80ft, the goal rate starts to increase again. This depicts that number of shots taken at a higher distance from the net is less and the number of goals scored at a higher distance from the net is more.

**The histogram of goal rate, i.e. #goals / (#no_goals + #goals), binned by angle is shown below:**

![m2_q2_2(2)](..\public\m2_q2_2(2).png)

The goal rate is the highest when the angle from the net is less than 1 degrees and is the lowest when the angle from the net is in the range of -90 and -70 and from 50-80 degrees. This depicts that number of shots taken at a higher angle from the net is less and the number of goals scored at a higher angle from the net is more.

**Question3: Knowing this, create another histogram, this time of goals only, binned by distance, and separate empty net and non-empty net events. Include this figure in your blogpost and discuss your observations. Can you find any events that have incorrect features (e.g. wrong x/y coordinates)? If yes, prove that one event has incorrect features.**

**Answer:**


**The histogram of goals only, binned by distance, and separate empty net and non-empty net events is shown below:**

![m2_q3](..\public\q2-3a.png)
This is the histogram of goals only in empty net and non-empty net. To see each one more in detail, the following figures
are histogram of goals only in empty net and in non-empty net in separate figures.

![m2_q3(2)](..\public\q2-3c.png)
![m2_q3(2)](..\public\q2-3b.png)

It is seen that in a non-empty net, most of the goals were from less then 60ft distance from the net and the most goals in this case
were from about 10- 20 ft distance from the net with more than 9000 occurrence. It makes sense
because it is so unlikely to score a goal from a far distance when the other team's net is not
empty.

In the empty net case, again the best distance was about 10-20ft.
 However, unlike the previous case that there were almost no goals
from over 60ft distance, here we can see many goals even from the defensive zone.

It can be seen that there are some events at the distance of more than 150ft
which have been goal in a non-empty net. It does not seem
right, as the goal was scored from the defensive zone, while
the net was not even empty.
So we printed
the goals in an empty net, where the distance of the shot was more than 150ft.
There were 104 events of such case. We decided to search for the video of the last game in the list,
with the game_id of 2019021001. This game had 2 goals which were in this list, one scored in 2nd period at 13:14 by LAK team
and the other one scored in 4th period at 01:58 by LAK team. We checked both the goals
and the result will be discussed below:

[The link for the video of the game](https://www.nhl.com/video/recap-lak-2-njd-1-fot/t-277350912/c-5313373?q=2019021001)

According to the
video in the link above, in the 2nd period there was a goal for LAK team which
was scored from a distance so close to the NJD team net. However, in the dataset we have,
it says the goal shot was from 166.07 ft distance with coordinates of x = -77.0 and y = 5. So we think
that the signs of the coordinates of this event are wrong. They should be x = 77, y = -5.

Checking the 2nd goal, we see that the goal was scored by LAK team from near the NJD team net.
However, in our dataset we see that is shows the shot_distance to be 154.72 ft with coordinates of
x = -65 and y = 15. It means that the goal was scored from the LAK rink side which is different
from what is seen in the video. So again, the coordinates are signed wrongly. The correct coordinates
would be x = 65 and y = -15.


![q2-weird df](..\public\weird_df.png)

In both cases we say that the y is also signed wrongly,
because looking at the video, it is seen that
when the positive x coordinates is on the right side of the video screen,
the goal is scored
from the lower parts of the rink(negative y axis)

Another possibility would be that the rink side of the teams are
set wrongly in the dataset, but as we checked the dataset we saw that the teams
did switch their side after each period. So if the rink side is set wrongly for
one period, it means it will be wrong for all the periods. Therefore there will still be
some goals on the non-empty net from the far distance. For example in the game we mentioned,
it can be seen that she shot distances for 1st period are short (less than 100 ft ) which makes sense. But
if we change the rink sides, then the shot distances will be mostly above 100 which 
looks wrong.
In the game we checked, just 1st period had correct features. The 2nd, 3rd and 4th
period mostly had large shot_distance values(more than 100ft) which is so different from
the other games having correct features.

So we believe the coordinates signs are the problem here, not the rink side.


## <span style="color:SlateBlue;">Q3. Baseline Models </span>

**Question1: Using your dataset (remember, don‚Äôt touch the test set!), create a training and validation split however you feel is appropriate. Using only the distance feature, train a Logistic Regression classifier with the completely default settings. Evaluate the accuracy (i.e. correctly predicted / total) of your model on the validation set. What do you notice? Look at the predictions and discuss your findings. What could be a potential issue? Include these discussions in your blog post.**

**Answer:**

The accuracy we got is 90.5%. The problem is that all the predicted values have the value as 0 which means that the model is not able to predict the goals and is only performing to predict the no-goals. This is because the goals are much less than no-goals in our dataset and therefore the dataset is imbalanced. So, we need to balance the dataset. Also all the metrics value such as f1 score, precision, recall values are 0.

**Question2:  Train two more Logistic Regression classifiers using the same setup as above, but
this time on the angle feature, and then both distance and angle. Produce the same
three curves as described in the previous section for each model. Including the random
baseline, you should have a total of 4 lines on each figure:**

**- Logistic Regression, trained on distance only (already done above)**

**- Logistic Regression, trained on angle only**

**- Logistic Regression, trained on both distance and angle**

**- Random baseline: rather than training a classifier, the predicted probability is
sampled from a uniform distribution, i.e. ùë¶
~
ùëñ
‚àº ùëà(0, 1)**


**Include these four figures (each with four curves) in your blogpost. In a few
sentences, discuss your interpretation of these results.**

**You will produce four figures (one curve per model per plot) to probe our model‚Äôs performance. Make sure you are using the
probabilities obtained on the validation set:**

**- a: Receiver Operating Characteristic (ROC) curves and the AUC metric of the ROC
curve. Include a random classifier baseline, i.e. each shot has a 50% chance of
being a goal.**

**- b: The goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability
model percentile, i.e. if a value is the 70th percentile, it is above 70% of the data.**

**- c: The cumulative proportion of goals (not shots) as a function of the shot
probability model percentile.**

**- d: The reliability diagram (calibration curve). Scikit-learn provides functionality to
create a reliability diagram in a few lines of code; check out the
CalibrationDisplay API (specifically the .from_estimator() or
.from_predictions() methods) for more information.**

**Answer:**

![q3-a](..\public\q3-a.png)

The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes. It is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes. When the AUC is between 0.5 and 1, there is a high chance that the classifier will be able to distinguish the positive class values from the negative class values. 
In our case, we can see that the curve of our classifier with Distance only and the curve of the classifier with Distance and Angle overlap. So it means that using the Angle feature was redundant because it did not improve our model. Also it is seen that the classifier with Distance and Angle ahs AUC metric of about 0.68 which is the best among these four.The classifier with Angle has AUC metric of almost the same as the Random model.

![q3-b](..\public\q3-b.png)

The higher the shot probability model percentile in a bin, the greater the chance of having higher goal rate i.e. an actual goal (ground truth) being in that bin. This can be validated by observing the from feature - distance and angle combined and distance only feature. However, the angle feature follows the trend similar to the random baseline, making it less significant since it does not add any value to the models performace.

![q3-c](..\public\q3-c.png)

For cumulative shot probability also, we can observe that the tren line for distance and angle feature and distance only feature is overlapping. The angle feature is following the trend of random baseline once again making it irrelevant when considered separately.

![q3-d](..\public\q3-d.png)

For a perfectly calibrated reliability curve, the mean probabilities in a bin should match with the proportion of positives in that bin. Examining the calibration plots corresponding to our selected features, model with distance and angle feature and the model with distance only feature have the best performance among all.
The model with angle feature is not very deterministic and follows random baseline.

**Links to the 3 models in Comet.ml**

[Link to the LR model with distance only](https://www.comet.com/rachel98/ift-6758-team-8/56f84ce084f646e1b316735a8c1e2706)

[Link to the LR model with angle only](https://www.comet.com/rachel98/ift-6758-team-8/225a346c66b64f53b8b9f1d817f16d3c)

[Link to the LR model with distance and angle](https://www.comet.com/rachel98/ift-6758-team-8/671bc6089bb44b3697e360a92175b59d)


## <span style="color:SlateBlue;">Q4. Feature Engineering II</span>
**Question: In your blogpost, add a list of all of the features that you created for this section. List each feature by both the column name in your dataframe AND a simple human-readable explanation (i.e. game_sec: Total number of seconds elapsed in the game). If you created any novel features, briefly describe what they are. Add a link to the experiment which stores the filtered DataFrame artifact for the specified game.**

First we changed our previous tidy data file, so we could have all the events, instead of having only shot and goal events.
Then using the new data produced, and using the features created section2(Feature Engineering 1) we created the following features:

- last_event_type: It is the type of the previous event.
- last_event_coordinates_x: It is the x coordinates of the previous event.
- last_event_coordinates_y: It is the y coordinates of the previous event.
- time_from_last_event(s): It is the time passed(in seconds) since the previous event.
- distance_from_last_event: It is the Euclidean distance of current event and previous event (distance of current event from the previous event).
 - rebound: Its a bool and specifies whether the previous event was a shot or not. If yes, it is True, else it is False
 - angle_change: If the shot is rebound, angle_change shows the amount of change in the angle compared with the previous event. It will be 0 if the shot is not a rebound.

- Speed: We calculate the speed of previous event. It is "distance_from_last_event" divided by "time_from_last_event(s)".

    [link to the experiment](https://www.comet.com/rachel98/ift-6758-team-8/aa9f03ac84e641f0991db8ce8415d4bb)

### Features created for bonus part:
- away_number_of_players: Number of non-goalie players on the ice for away team.
- home_number_of_players: Number of non-goalie players on the ice for home team.
- time_since_power_play: It is the time passed(in seconds) since the powerplay started. It will be 0 when the powerplay ends.
As penalties are cancelled after specific times, and we also have some delayed penalties, I used the dataframe which has all the events in it(instead of the one used in Feature engineering 2). because more events and times could be seen and therefore the progress of penalties are better seen.

    [link to the experiment of bonus](https://www.comet.com/rachel98/ift-6758-team-8/992eadb2880d4a44b85e86b9c434276c)

### A breif explanation on a part of the results of this experiment

As seen in the dataframe in the link provided, the first penalty is occured at 3:18 for team Winnipeg Jets (index 26 of df) and therefore powerplay starts and number of players for this team will be 4. It is a minor penalty(2 minutes long) so it should last until 5:18 (If no goal is scored by the other team during this time).
Winnipeg Jets gets another minor penalty at 3:53. So they will have 3 players and the powerplay still goes on. This penalty will expire at 5:53.

At 4:35 Washington Capitals scores a goal, and therefore the first penalty of Winnipeg Jets will be over. So the players of Winnipeg Jets will be 4. But the powerplay still goes on.

At 5:10 (index 42 of df )Washington Capitals gets a minor penalty and therefore its number of players will be 4.
Now the teams are on even strength and therefore the powerplay start time will end immediately as seen in the dataframe. This penalty should end on 7:10.

At 5:33 Winnipeg Jets scores a goal and therefore the player of Washington Capitals who was serving his minor penalty will be back to ice. Now another powerplay will begin.

At 5:53 the penalty of Winnipeg Jets ends and they will be back to 5 players and the powerplay will be reset. These changes take place in index 50 of the dataframe, at 7:10. Because 7:10 is the first event we have after period time of 5:53. (index 49 is for an event at 5:45 and index 50 is for an event happening at 7:10)

## <span style="color:SlateBlue;">Q5. Advanced Models</span>
**Question1: Add the corresponding curves to the four figures in your blog post. Briefly (few sentences) discuss your training/validation setup, and compare the results to the Logistic Regression baseline. Include a link to the relevant comet.ml entry for this experiment.**

**Answer:**

![q5_1(1)](..\public\q5_1(1).png)
![q5_1(2)](..\public\q5_1(2).png)
![q5_1(3)](..\public\q5_1(3).png)
![q5_1(4)](..\public\q5_1(4).png)

The training and validation sets are divided in the ratio of 80 to 20. Simple XGBoost without any changes in hyperparameters, we got an accuracy of 71% on the validation set. The accuracy is less than the baseline model. The reason for this is that the dataset is imbalanced and the model is not able to predict the goals in case of Logistic Regression. Link to the experiment in comet.ml is as follows:

[Link to the model](https://www.comet.com/rachel98/ift-6758-team-8/ff9a12d70a4144ab9aa4b486a38c18be)

**Question2:  In your blog post, discuss your hyperparameter tuning setup, and include figures to substantiate your choice of hyperparameters. Once tuned, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant comet.ml entry for this experiment, and log this model to the model registry.**

**Answer:**

The data was preprocessed to perform the XGBoost more efficiently as the data was changed in Question2 in comparison to question1. The data used was the result from feature engineering 2. The hyperparameters are then tuned using the RandomizedSearchCV and stratifiedKFold. The hyperparameters are tuned using the following parameters:
- n_estimators: Number of trees in the forest
- max_depth: Maximum depth of the tree
- learning_rate: Learning rate
- gamma: Minimum loss reduction required to make a further partition on a leaf node of the tree
- min_child_weight: Minimum sum of instance weight(hessian) needed in a child
- subsample: Subsample ratio of the training instance
- colsample_bytree: Subsample ratio of columns when constructing each tree

The best hyperparameters gave us the following results:
```
The confusion matrix is :  [[55119   300]
                            [  851  3354]]
The accuracy score is:  0.9806956930095264
The classification report:
                precision    recall  f1-score   support
           0       0.98      0.99      0.99     55419
           1       0.92      0.80      0.85      4205

    accuracy                           0.98     59624
   macro avg       0.95      0.90      0.92     59624
weighted avg       0.98      0.98      0.98     59624
```
The plots for best XGBoost model after hypertuning is as follows:
![q5_2(1)](..\public\q5_2(1).png)
![q5_2(2)](..\public\q5_2(2).png)
![q5_2(3)](..\public\q5_2(3).png)
![q5_2(4)](..\public\q5_2(4).png)

The accuracy of the model is 98% which is much better than the baseline model. The model is able to predict the goals and no-goals with a good accuracy. The link to the experiment in comet.ml is as follows:

[Link to the model](https://www.comet.com/rachel98/ift-6758-team-8/23bdaba29b71404faf77ac0c1f998690)

**Question3: Discuss the feature selection strategies that you tried, and what was the most optimal set of features that you came up with. Include some figures to substantiate your claims. Include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant comet.ml entry for this experiment, and log this model to the model registry.**

**Answer:**
The feature selection was done using the plotting of the feature importance. To demonstrate via figures:
![q5_3(1)](..\public\q5_3(1).png)
![q5_3(2)](..\public\q5_3(2).png)
![q5_3(3)](..\public\q5_3(3).png)

The optimal set of features that we came up with is as follows:
- last_event_coordinates_x
- last_event_coordinates_y
- distance_from_last_event
- speed
- period_time
- shot_distance
- shot_angle

The most coorelated features seems to be the shot_distance and shot_angle. The result of the best model after feature selection was :
```
Confusion matrix:  [[55142   277]
                    [  899  3306]]
Accuracy Score:  0.9802763987655978
The classification report:
                precision    recall  f1-score   support

           0       0.98      1.00      0.99     55419
           1       0.92      0.79      0.85      4205

    accuracy                           0.98     59624
   macro avg       0.95      0.89      0.92     59624
weighted avg       0.98      0.98      0.98     59624
```

The plots for the best model after feature selection is as follows:

![q5_3(4)](..\public\q5_3(4).png)
![q5_3(5)](..\public\q5_3(5).png)
![q5_3(6)](..\public\q5_3(6).png)
![q5_3(7)](..\public\q5_3(7).png)

The XGBoost Baseline model had an accuracy of 71% and the model after feature selection has an accuracy of 98%. The model is able to predict the goals and no-goals with a good accuracy because of the hyperparameter tuning and best features that we got after the feature selection. The link to the experiment in comet.ml is as follows:

[Link to the model](https://www.comet.com/rachel98/ift-6758-team-8/1d45ba20488342128a2faf201d4814ab)

## <span style="color:SlateBlue;">Q6. Give it your Best Shot </span>
**Question1: In your blog post, discuss the various techniques and methods you tried. Include the same four figures as in Part 3 (ROC/AUC curve, goal rate vs probability percentile, cumulative proportion of goals vs probability percentile, and the reliability curve).Make sure to include and highlight what you consider your best ‚Äòfinal‚Äô model.**

The training and validation sets are divided in the ratio of 80 to 20. We have leveraged **3 different models i.e. Decision Trees, Random Forest and Multi Layer Perceptron model** and have performed a comparative study for **8 different experiments and strategies** using **PCA, GridSearchCV and RandomizedSearchCV**. The results and explaination for each one of them can be found below.

**Decision Trees (DTs)** are a non-parametric supervised learning method which are used to predict the value of a target variable i.e. goal by learning simple decision rules inferred from the data features. The logic behind the decision tree can be easily understood because it shows a tree-like structure. In a decision tree, for predicting the class of the given dataset, the algorithm starts from the root node of the tree. This algorithm compares the values of root attribute with the record (real dataset) attribute and, based on the comparison, follows the branch and jumps to the next node. For the next node, the algorithm again compares the attribute value with the other sub-nodes and move further. It continues the process until it reaches the leaf node of the tree.
 
Experiment 1:

Hyperparameters Used:
criterion : The function to measure the quality of a split. Used "gini
splitter: The strategy used to choose the split at each node. Used "best".
min_samples_split: The minimum number of samples required to split an internal node. Used value as 2.
ccp_alphanon: Complexity parameter used for Minimal Cost-Complexity Pruning. Used 0 value.
max_features: The number of features to consider when looking for the best split. Used all the features i.e. n_features.

The experimented yielded the following results with accuracy of 0.83 and auc score of 0.54. Below is the classification report provided by this model:

```
precision    recall  f1-score   support

           0       0.91      0.90      0.91     55414
           1       0.16      0.19      0.18      5758

    accuracy                           0.83     61172
   macro avg       0.54      0.54      0.54     61172
weighted avg       0.84      0.83      0.84     61172
```

Experiment 2:
We have leveraged GridSearchCV for performing hyperparameter tuning in order to determine the optimal values for a given model. Scikit-learn‚Äôs(or SK-learn) library is used for GridSearchCV function. This function helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, we can select the best parameters from the listed hyperparameters. GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method. Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose the one with the best performance.

1.estimator: the model instance for which we want to check the hyperparameters.
2.params_grid: the dictionary object that holds the hyperparameters
3.scoring: evaluation metric that we want to use
4.cv: number of cross-validation 

Using this we found the best hyperparameters for the Decision Tree Classifier as ccp_alpha=1e-09, criterion='entropy', max_features='auto' and min_samples_split=2. The model resulted in an accuracy of 0.90 and auc score of 0.50. Below is the classification report provided by this model:

```
precision    recall  f1-score   support

           0       0.91      1.00      0.95     55414
           1       0.00      0.00      0.00      5758

    accuracy                           0.91     61172
   macro avg       0.45      0.50      0.48     61172
weighted avg       0.82      0.91      0.86     61172
```

Disadvantage of using Decision Trees: In case of unbalanced dataset (which is the scenario for us, as we have less no of goals), decision trees create biased trees.

**Random Forest** is made up of a collection of decision trees, and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.

Experiment 1:

We have built randm forest with 100 estimators, gini criterion to measure the quality of the split, sqrt(n_features) as the max features to consider when looking for the best split. Additionally, we have also computed top 20 important features using feature_importances_ attribute and they are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree. The graph below shows the top 20 important features:

![m2_q6_imp](..\public\feature_imp.png)

The experimented yielded the following results with accuracy of 0.905 and auc score of 0.509. Below is the classification report provided by this model:
```
                precision    recall  f1-score   support

           0       0.91      1.00      0.95     55414
           1       0.43      0.02      0.04      5758

    accuracy                           0.91     61172
   macro avg       0.67      0.51      0.50     61172
weighted avg       0.86      0.91      0.86     61172   
```

Experiment 2:
In this experiment, we have used Principal component analysis (PCA), a statistical technique to convert high dimensional data to low dimensional data by selecting the most important features that capture maximum information about the dataset. The features are selected on the basis of variance that they cause in the output. The feature that causes highest variance is the first principal component.

We have leveraged this technique to reduce the dimensionality of the data and also to get rid of all collinear features, as all collinear features will end up in a single PCA component. Random Forest with these hyperparameters is build - n_estimators=100, max_depth=10, max_features='auto'. Pipeline form sklearn package is used which sequentially applies a list of transforms and a final estimator. This final estimator only needs to implement fit.

The experimented yielded the following results with accuracy of 0.905 and auc score of 0.502. Below is the classification report provided by this model:
```
               precision    recall  f1-score   support

           0       0.91      1.00      0.95     55414
           1       0.51      0.01      0.01      5758

    accuracy                           0.91     61172
   macro avg       0.71      0.50      0.48     61172
weighted avg       0.87      0.91      0.86     61172
```

**MultiLayer Perceptron (MLP)** is a feedforward artificial neural network model that maps input data sets to a set of appropriate outputs. An MLP consists of multiple layers and each layer is fully connected to the following one. The nodes of the layers are neurons with nonlinear activation functions, except for the nodes of the input layer. Between the input and the output layer there may be one or more nonlinear hidden layers. 

Experiment 1:

Hyperparameters:

1. hidden_layer_sizes : With this parameter we can specify the number of layers and the number of nodes we want to have in the Neural Network Classifier. Each element in the tuple represents the number of nodes at the ith position, where i is the index of the tuple. Thus, the length of the tuple indicates the total number of hidden layers in the neural network. We have set this to (100,100,100).
2. max_iter: Indicates the number of epochs. We have set this to 1000.
3. activation: The activation function for the hidden layers. We have set this to relu.
4. solver: This parameter specifies the algorithm for weight optimization over the nodes. We have set this to Adam.

The experimented yielded the following results with accuracy of 0.91 and auc score of 0.50. Below is the classification report provided by this model:
```
               precision    recall  f1-score   support

           0       0.91      1.00      0.95     55414
           1       0.00      0.00      0.00      5758

    accuracy                           0.91     61172
   macro avg       0.45      0.50      0.48     61172
weighted avg       0.82      0.91      0.86     61172
```

Experiment 2:
We have leveraged PCA technique to reduce the dimensionality of the data and also to get rid of all collinear features, as all collinear features will end up in a single PCA component. MLP Classifier with above set of hyperparameters is used. Pipeline form sklearn package is used which sequentially applies a list of transforms and a final estimator. This final estimator only needs to implement fit.

The experimented yielded the following results with accuracy of 0.91 and auc score of 0.50. There was no significant improvement in the results. Below is the classification report provided by this model:
```
              precision    recall  f1-score   support

           0       0.91      1.00      0.95     55414
           1       0.00      0.00      0.00      5758

    accuracy                           0.91     61172
   macro avg       0.45      0.50      0.48     61172
weighted avg       0.82      0.91      0.86     61172
```

Experiment 3:
In this we have leveraged RandomizedSearchCV along with MLP and PCA, which implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:

1. A budget can be chosen independent of the number of parameters and possible values.
2. Adding parameters that do not influence the performance does not decrease efficiency.

Below is the dictionary for the hyperparameter search that we have considered.

```
param_grid = {
        'clf__hidden_layer_sizes': [ (100,100), (100,)],
        'clf__activation': ['relu', 'tanh', 'logistic'],
        'clf__solver': ['adam', 'sgd', 'lbfgs'],
        'clf__alpha': [0.0001, 0.05],
        'clf__learning_rate': ['constant','adaptive'],
    }
```
The experimented yielded the following results with accuracy of 0.91 and auc score of 0.50. There was no significant improvement in the results. Below is the classification report provided by this model:

```
              precision    recall  f1-score   support

           0       0.91      1.00      0.95     55414
           1       0.00      0.00      0.00      5758

    accuracy                           0.91     61172
   macro avg       0.45      0.50      0.48     61172
weighted avg       0.82      0.91      0.86     61172
```
The plots for a comparative study of the performance are provided below. Looking at these plots we can easily deduce that the best model is Random Forest Tuned i.e. experiment 2 with PCA.

![m2_q6_roc](..\public\best_shot_roc.png)
![m2_q6_goal](..\public\best_shot_goal_rate.png)
![m2_q6_cum_prob](..\public\best_shot_cum_prob.png)
![m2_q6_rel](..\public\best_shot_relability.png)

**Question2: include links to the experiment entry in your comet.ml projects that you included quantitative metrics for (around 3-4). Log the models to the experiments on comet.ml.**

[Link to the RF model](https://www.comet.com/rachel98/ift-6758-team-8/75c1beaccc0d43e3bcb910ec5e723a30)

[Link to the RF Tuned model](https://www.comet.com/rachel98/ift-6758-team-8/969bcdffa6fd45a0a588707528fb2613)

[Link to the DT model](https://www.comet.com/rachel98/ift-6758-team-8/909fab46dcae4b86be561a753e0f6a27)

[Link to the DT Tuned model](https://www.comet.com/rachel98/ift-6758-team-8/59a52c4f856f49f58e52de6713be7db3)

[Link to the MLP model](https://www.comet.com/rachel98/ift-6758-team-8/07290570329f49f5b772a172f49cff52)

[Link to the MLP Tuned model](https://www.comet.com/rachel98/ift-6758-team-8/c3a52601129842aca0880ed802ef70d9)

[Link to the MLP Tuned RandomizedSearchCV model](https://www.comet.com/rachel98/ift-6758-team-8/c28cdddc464144109d517baedb0c6801)

## <span style="color:SlateBlue;">Q7. Evaluate on test Set </span>

**Question1: Test your 5 models on the untouched 2019/20 regular season dataset. In your blogpost, include the four figures described above. Discuss your results and observations on the test set. Do your models perform as well on the test set as you did on your validation set when building your models? Do any models perform better or worse than you expected?**

**Answer:**

![q7_1_1](..\public\q7_1_1.png)
![q7_1_2](..\public\q7_1_2.png)
![q7_1_3](..\public\q7_1_3.png)
![q7_1_4](..\public\q7_1_4.png)

The models performed best on the Random Forest followed by the XGBoost model. The model performed better on validation set than the test set. The best model was the Random Forest model with an accuracy of 98% on the validation set and 91% on the test set. Followed by Random forest, was the XGBoost model and then was the Logistic Regression. The model performed better on validation set than the test set. 

As seen by the graphs above, Random Forest and XGBoost models performed better than the Logistic Regression model. The Random Forest model performed better than the XGBoost model. The Logistic Regression model performed better than the baseline model. There is a satisfactory result in terms of generalization looking into the accuracy on test and validation set.

**Question2: Test your 5 models on the untouched 2019/20 playoff games. In your blogpost, include the four figures described above. Discuss your results and observations on this test set. Are there any differences to the regular season test set or do you get similar ‚Äògeneralization‚Äô performance?**

**Answer:**

![q7_2_1](..\public\q7_2_1.png)
![q7_2_2](..\public\q7_2_2.png)
![q7_2_3](..\public\q7_2_3.png)
![q7_2_4](..\public\q7_2_4.png)

The models perform best on XGBoost followed by Random Forest. The model performed better on validation set than the test set.  Followed by XGBoost, was the Random Forest model and then was the Logistic Regression. Hence, we can see that in playoff data was best performed by XGBoost whereas in regular season data was best performed by Random Forest. 

As seen by the graphs above, XGBoost and Random Forest models performed better than the Logistic Regression model. The XGBoost model performed better than the Random Forest model. The Logistic Regression model performed better than the baseline model.
The generalization performance is similar to the regular season test set as the models performed better on the validation set than the test set. The overlapping of curves of XGBoost and Random Forest suggest that both the models perform almost similarly on the test set.
