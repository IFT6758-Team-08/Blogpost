---
layout: post
title: Milestone 1
---


## <span style="color:SlateBlue;">Q1: Data Aquisation</span>
The Data Aquisation code in written in the get_data.py file which consists of a few functions that I will discuss each below:



**`get_data_regular(year, file_path)`**





This function takes as input a year representing a season and a file path and downloads all the liveData and GameData events of the regular season in the file_path.
It first checks whether the data for the requested season exists or not: If it is already downloaded in the file_path, it will just load it. Else it will start downloading the data.
### How does it download the requested data?

 1. First we need to specify the GAME_ID in the following URL to be able to download the data:
 https://statsapi.web.nhl.com/api/v1/game/[GAME_ID]/feed/live/

	We know that GAME_ID for a regular season is created like this:
	4 digits for the season year + 02 + 4 digits for specific game number.
So the first game of the season 2016-2017 will have GAME_ID of 2016020001.

	This is how we create the GAME_ID in this function:

	    GAME_ID = "{}02{:04d}".format(year, i)

	i is specific number of the game.

	Using this, we can create the GAME_IDs for each season and we only need to increase the GAME_ID by 1 to go to the next game.


	#### When do we know if we have downloaded all the games?

	We stop downloading data from a given season when we reach a GAME_ID that has no liveData in its URL. It means that game is not available, and so other game s with higher GAME_IDs will not be available as well.

 2. Now that we know what GAME_ID to use, we will start downloading.
Using the GAME_ID we have, we can substitude it in the URL we have:

		`url='https://statsapi.web.nhl.com/api/v1/game/{}/feed/live/'.format(GAME_ID)`

	We can now download the data in this URL using the ***request*** library and save it as a json file.

		r = requests.get(url).json()

	This way we can have all the data in the url in r.



 3. In this part we can check to see whether liveData exists for this 			GAME_ID or not.  As said before, if it doesnt, we should end our downloading. But if it does, we save the data in a dictionary called **all_regular_games** with the key being GAME_ID.

	 We dont save all the r data we have as we don't need many parts. We just save gameData and liveData from each valid url and save them as in a dictionary called **regular_game** as below:


		regular_game['gameData'], regular_game['liveData'] = r['gameData'], r['liveData']
	Them we add this dictionary as value of the GAME_ID key to our main dictionary called **all_regular_games**:


	    all_regular_games[GAME_ID] = regular_game



At the end when we finish downloading all we needed, we now have all the data in **all_regular_games** dictionary.
Now we should just save this data in a json file using **json** library.

    with open(file_path, 'w') as f_regular:
	    json.dump(all_regular_games, f_regular)


The second function works the same as the first function, but it downloads the data for the playoff games.

**`def get_data_playoffs(year, file_path)`**

All the steps for this function is the same as the **get_data_regular** function. But there is a little difference in the way we compute GAME_ID which i will discuss below:
We know that GAME_ID for a playoff game is created like below:

4 digits for the season year + 030 + 3 digits for specific number of the game.
Specific number of the game is created like this:
1 digit for the round of the playoffs + 1 digit for the matchup + 1 digit for the game (out of 7)
So the first game of the first matchup of the first round of the season 2016-2017 will have GAME_ID of 2016030111

We know that there are 4 rounds in each playoff. So the first digit of the specific game number will be from 1 to 4.
We also know that in the first round we will have 16 teams to compete. So there will be 8 matchups(2^(4-1)).
Then for the second round we have 8 teams to compete. So there will be 4 matchups(2^(4-2)).
For the thirds and forth round we have 4 and 2 teams respectively ans therefore there will be 2 matchups for the third round(2^(4-3)) and 1 matchup for the last round(2^(4-4)).
So we can define the maximum number of matchups available for each round as below:

    2 ** (4 - number of round)

  So this way we can specify the range available for the second digit of our specific game number to be from 1 to (2 ** (4 - number of round)).

Finally we know that maximum number of the games in each matchup is 7. So the third digit in the specific game number will be from 1 to 7.

Now we can easily make GAME_IDs for a playoff game and download their URL like before:
 We should just go through a for loop from **1 to 4 as the round number**(for the first digit of the specific game number), and have another for loop **from 1 to (2 ** (4 - number of round)) for number of the matchup** (the second digit of the specific game number) and the last loop **from 1 to 7 specifying number of the game** in that matchup( third digit of the specific game number). In side the third loop we will create the GAME_ID and the url like below:


    GAME_ID = '{}030{}{}{}'.format(year, p_round, match_up, game)
	url = 'https://statsapi.web.nhl.com/api/v1/game/{}/feed/live/'.format(GAME_ID)

p_round is the round number, match_up is the matchup number and game is the game number in that matchup.

We can now download our data using **request** library and if the liveData exists, we save the data in the dictionary format explained before(we will save data in **all_playoff_games** dictionary). If not, we will skip saving and will move on to the next itereation available.

At the final step we save the **all_playoff_games** to a json file like before.

Now we just need a main function to make everything work together

    main()

  We specify the following variables:


 - path: is the path where our downloaded files will be saved
 - year: is the start year
 - to_year: is the last year

  So when we want to download data of the years 2016 to 2021 we should specify year = 2019 and to_year = 2021.

  Then in a for loop in the range of (year, to_year+1) we call :
  We make a name for the our output file.
  For regular season data in year "y" we have this name:

    file_path_regular = path + "/" + str(y) + "_regular_season.json"
 And for a playoff game data in the year "y" it will be:


    file_path_playoff = path + "/" + str(  y) + "_playoffs.json"
And at the last step we call the 2 functions we wrote:

    get_data_playoffs(y, file_path_playoff)
    get_data_regular(y, file_path_regular)


